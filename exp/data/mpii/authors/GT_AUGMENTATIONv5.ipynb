{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758feb18-f30f-48e0-a511-8e0dae56d385",
   "metadata": {},
   "outputs": [],
   "source": [
    "####Updated/Reduced Categories Name, Test Cases Names and Numbers, Added Retry, Sleep Time, File Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8ad6c2-d396-4482-a076-122c33fd4830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Base GT - ChatGPT\n",
    "# -------------------------------\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import csv\n",
    "import openai\n",
    "import json\n",
    "import sys\n",
    "import time\n",
    "\n",
    "openai.api_key = \"placeholder\"  # Replace with your actual API key\n",
    "\n",
    "MAX_RETRIES = 3  # Number of retries on API failure\n",
    "\n",
    "def get_gpt4_response(prompt, model=\"gpt-4o\", temperature=1, max_tokens=10000):\n",
    "    \"\"\"\n",
    "    Sends a prompt to GPT-4o and returns the response content or raises an Exception.\n",
    "    \"\"\"\n",
    "    #print(\"\\n[GPT-4o] Sending prompt...\")\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens,\n",
    "    )\n",
    "    content = response['choices'][0]['message']['content'].strip()\n",
    "    #print(\"[GPT-4o] Received response:\")\n",
    "    #print(content)\n",
    "    return content\n",
    "\n",
    "def process_csv_files(folder_path, output_json_file):\n",
    "    \"\"\"\n",
    "    Process all CSV files in a folder. For each CSV:\n",
    "      1) Read the \"Description\" column in order.\n",
    "      2) Construct a single prompt.\n",
    "      3) Call GPT-4o with retries on error.\n",
    "      4) Save partial progress in a JSON file so the script can be resumed.\n",
    "    \"\"\"\n",
    "    # 1) Load existing results (if any) so we can resume from partial progress.\n",
    "    if os.path.exists(output_json_file):\n",
    "        with open(output_json_file, \"r\", encoding=\"utf-8\") as jf:\n",
    "            try:\n",
    "                results = json.load(jf)\n",
    "            except json.JSONDecodeError:\n",
    "                # If the file is empty or invalid, start with an empty dictionary\n",
    "                results = {}\n",
    "        print(f\"Loaded existing results from '{output_json_file}'.\")\n",
    "    else:\n",
    "        results = {}\n",
    "        print(f\"No existing '{output_json_file}' found. Starting fresh.\")\n",
    "    \n",
    "    # 2) Get all CSV files\n",
    "    csv_files = sorted(glob.glob(os.path.join(folder_path, \"*.csv\")))\n",
    "    print(f\"Found {len(csv_files)} CSV file(s) in folder '{folder_path}'.\")\n",
    "\n",
    "    # 3) Process each CSV file\n",
    "    for index, csv_file in enumerate(csv_files, start=1):\n",
    "        # Check if we already have a result for this index (resume logic)\n",
    "        if str(index) in results:\n",
    "            print(f\"Skipping file {index} ('{csv_file}') - already in JSON results.\")\n",
    "            continue\n",
    "        \n",
    "        #print(f\"\\nProcessing file {index}: '{csv_file}'\")\n",
    "        descriptions = []\n",
    "\n",
    "        # Read the CSV and extract the \"Description\" column\n",
    "        with open(csv_file, newline='', encoding='utf-8') as f:\n",
    "            # Adjust delimiter if needed (\",\" or \"\\t\", etc.)\n",
    "            reader = csv.DictReader(f, delimiter=\",\")\n",
    "            for row_number, row in enumerate(reader, start=1):\n",
    "                if \"Description\" in row:\n",
    "                    description = row[\"Description\"]\n",
    "                    #print(f\"File {index}, Row {row_number} - Description: {description}\")\n",
    "                    descriptions.append(description)\n",
    "                else:\n",
    "                    print(\n",
    "                        f\"Warning: 'Description' column not found in row {row_number} of {csv_file}\"\n",
    "                    )\n",
    "\n",
    "        # Build the prompt\n",
    "        prompt = (\n",
    "            \"I have a list of scenes describing a movie step by step. \"\n",
    "            \"Please transform them into a single paragraph, preserving the chronological order. \"\n",
    "            \"Include every detail from each scene without adding or omitting any information. \"\n",
    "            \"Use only straightforward, factual wording, and avoid any new or descriptive language beyond what is provided in the scene notes.\\n\\n\"\n",
    "            \"Input: \\tDescription\\n\"\n",
    "        )\n",
    "        for i, desc in enumerate(descriptions, start=1):\n",
    "            prompt += f\"{i}\\t{desc}\\n\"\n",
    "\n",
    "        #print(f\"\\nConstructed prompt for file {index}:\\n{prompt}\")\n",
    "\n",
    "        # 4) Call GPT-4o with retries\n",
    "        attempt = 0\n",
    "        while attempt < MAX_RETRIES:\n",
    "            try:\n",
    "                response = get_gpt4_response(prompt)\n",
    "                # If we get here, we have a successful response\n",
    "                break\n",
    "            except Exception as e:\n",
    "                attempt += 1\n",
    "                #print(f\"Error on attempt {attempt} for file {index}: {e}\")\n",
    "                if attempt < MAX_RETRIES:\n",
    "                    #print(f\"Retrying (attempt {attempt+1}/{MAX_RETRIES})...\")\n",
    "                    time.sleep(3)  # short delay before retry\n",
    "                else:\n",
    "                    # Exceeded max retries, save partial progress and exit\n",
    "                    #print(\"Max retries exceeded. Saving partial progress and exiting.\")\n",
    "                    results[str(index)] = f\"Error after {MAX_RETRIES} attempts: {e}\"\n",
    "                    with open(output_json_file, \"w\", encoding='utf-8') as json_file:\n",
    "                        json.dump(results, json_file, indent=4, ensure_ascii=False)\n",
    "                    sys.exit(1)\n",
    "\n",
    "        # 5) Store result in dictionary\n",
    "        results[str(index)] = response\n",
    "        #print(f\"[File {index}] Stored GPT-4o response in results dictionary.\")\n",
    "\n",
    "        # 6) Save partial progress to JSON after each file\n",
    "        with open(output_json_file, \"w\", encoding='utf-8') as json_file:\n",
    "            json.dump(results, json_file, indent=4, ensure_ascii=False)\n",
    "        print(f\"Progress saved after file {index}.\")\n",
    "\n",
    "    #print(f\"\\nAll CSV files processed. Final results have been saved to '{output_json_file}'.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    folder_path = \"/home/jacks.local/hdubey/VADv6/experiment/rawGT3\"  # Update to your folder\n",
    "    output_json_file = \"GT.json\"\n",
    "\n",
    "    print(\"Starting CSV processing and GPT-4o querying...\")\n",
    "    process_csv_files(folder_path, output_json_file)\n",
    "    print(\"Processing complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a53374-aae1-4aea-b5e6-d45f2fa6cd04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e6293c-0d65-45bd-9979-50ea5e2abe86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82e300c-8c8f-47ae-898b-a9200b29f4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# Base GT - Gemini\n",
    "# -------------------------------------------\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import csv\n",
    "import openai\n",
    "import json\n",
    "import sys\n",
    "import time\n",
    "\n",
    "openai.api_key = \"placeholder\"  # Replace with your actual API key\n",
    "\n",
    "MAX_RETRIES = 3  # Number of retries on API failure\n",
    "\n",
    "def get_gemini_response(prompt, model=\"gemini\", temperature=1, max_tokens=10000):\n",
    "    \"\"\"\n",
    "    Sends a prompt to Gemini and returns the response content or raises an Exception.\n",
    "    \"\"\"\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens,\n",
    "    )\n",
    "    content = response['choices'][0]['message']['content'].strip()\n",
    "    return content\n",
    "\n",
    "def process_csv_files(folder_path, output_json_file):\n",
    "    \"\"\"\n",
    "    Process all CSV files in a folder. For each CSV:\n",
    "      1) Read the 'Description' column in order.\n",
    "      2) Construct a single prompt.\n",
    "      3) Call Gemini with retries on error.\n",
    "      4) Save partial progress in a JSON file so the script can be resumed.\n",
    "    \"\"\"\n",
    "    # 1) Load existing results (if any) so we can resume from partial progress.\n",
    "    if os.path.exists(output_json_file):\n",
    "        with open(output_json_file, \"r\", encoding=\"utf-8\") as jf:\n",
    "            try:\n",
    "                results = json.load(jf)\n",
    "            except json.JSONDecodeError:\n",
    "                results = {}\n",
    "        print(f\"Loaded existing results from '{output_json_file}'.\")\n",
    "    else:\n",
    "        results = {}\n",
    "        print(f\"No existing '{output_json_file}' found. Starting fresh.\")\n",
    "    \n",
    "    # 2) Get all CSV files\n",
    "    csv_files = sorted(glob.glob(os.path.join(folder_path, \"*.csv\")))\n",
    "    print(f\"Found {len(csv_files)} CSV file(s) in folder '{folder_path}'.\")\n",
    "\n",
    "    # 3) Process each CSV file\n",
    "    for index, csv_file in enumerate(csv_files, start=1):\n",
    "        # Check if result exists\n",
    "        if str(index) in results:\n",
    "            print(f\"Skipping file {index} ('{csv_file}') - already in JSON results.\")\n",
    "            continue\n",
    "        \n",
    "        descriptions = []\n",
    "        with open(csv_file, newline='', encoding='utf-8') as f:\n",
    "            reader = csv.DictReader(f, delimiter=\",\")\n",
    "            for row_number, row in enumerate(reader, start=1):\n",
    "                if \"Description\" in row:\n",
    "                    description = row[\"Description\"]\n",
    "                    descriptions.append(description)\n",
    "                else:\n",
    "                    print(\n",
    "                        f\"Warning: 'Description' column not found in row {row_number} of {csv_file}\"\n",
    "                    )\n",
    "\n",
    "        # Build the prompt (identical wording/punctuation as requested)\n",
    "        prompt = (\n",
    "            \"I have a list of scenes describing a movie step by step. \"\n",
    "            \"Please transform them into a single paragraph, preserving the chronological order. \"\n",
    "            \"Include every detail from each scene without adding or omitting any information. \"\n",
    "            \"Use only straightforward, factual wording, and avoid any new or descriptive language beyond what is provided in the scene notes.\\n\\n\"\n",
    "            \"Input: \\tDescription\\n\"\n",
    "        )\n",
    "        for i, desc in enumerate(descriptions, start=1):\n",
    "            prompt += f\"{i}\\t{desc}\\n\"\n",
    "\n",
    "        # 4) Call Gemini with retries\n",
    "        attempt = 0\n",
    "        while attempt < MAX_RETRIES:\n",
    "            try:\n",
    "                response = get_gemini_response(prompt)\n",
    "                break\n",
    "            except Exception as e:\n",
    "                attempt += 1\n",
    "                if attempt < MAX_RETRIES:\n",
    "                    time.sleep(3)  # short delay\n",
    "                else:\n",
    "                    results[str(index)] = f\"Error after {MAX_RETRIES} attempts: {e}\"\n",
    "                    with open(output_json_file, \"w\", encoding='utf-8') as json_file:\n",
    "                        json.dump(results, json_file, indent=4, ensure_ascii=False)\n",
    "                    sys.exit(1)\n",
    "\n",
    "        # 5) Store result\n",
    "        results[str(index)] = response\n",
    "\n",
    "        # 6) Save partial progress\n",
    "        with open(output_json_file, \"w\", encoding='utf-8') as json_file:\n",
    "            json.dump(results, json_file, indent=4, ensure_ascii=False)\n",
    "        print(f\"Progress saved after file {index}.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    folder_path = \"/home/jacks.local/hdubey/VADv6/experiment/rawGT3\"  # Update to your folder\n",
    "    output_json_file = \"GT_gemini.json\"\n",
    "\n",
    "    print(\"Starting CSV processing and Gemini querying...\")\n",
    "    process_csv_files(folder_path, output_json_file)\n",
    "    print(\"Processing complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702a3b25-9ac9-44e8-8c5b-73f8449c12b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79790f70-b248-4754-9cc1-60907707c7f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c181e3e0-aa70-4d07-b23a-229ad778e20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# Base GT - Claude\n",
    "# -------------------------------------------\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import csv\n",
    "import openai\n",
    "import json\n",
    "import sys\n",
    "import time\n",
    "\n",
    "openai.api_key = \"placeholder\"  # Replace with your actual API key\n",
    "\n",
    "MAX_RETRIES = 3  # Number of retries on API failure\n",
    "\n",
    "def get_claude_response(prompt, model=\"claude\", temperature=1, max_tokens=10000):\n",
    "    \"\"\"\n",
    "    Sends a prompt to Claude and returns the response content or raises an Exception.\n",
    "    \"\"\"\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens,\n",
    "    )\n",
    "    content = response['choices'][0]['message']['content'].strip()\n",
    "    return content\n",
    "\n",
    "def process_csv_files(folder_path, output_json_file):\n",
    "    \"\"\"\n",
    "    Process all CSV files in a folder. For each CSV:\n",
    "      1) Read the 'Description' column in order.\n",
    "      2) Construct a single prompt.\n",
    "      3) Call Claude with retries on error.\n",
    "      4) Save partial progress in a JSON file so the script can be resumed.\n",
    "    \"\"\"\n",
    "    if os.path.exists(output_json_file):\n",
    "        with open(output_json_file, \"r\", encoding=\"utf-8\") as jf:\n",
    "            try:\n",
    "                results = json.load(jf)\n",
    "            except json.JSONDecodeError:\n",
    "                results = {}\n",
    "        print(f\"Loaded existing results from '{output_json_file}'.\")\n",
    "    else:\n",
    "        results = {}\n",
    "        print(f\"No existing '{output_json_file}' found. Starting fresh.\")\n",
    "    \n",
    "    csv_files = sorted(glob.glob(os.path.join(folder_path, \"*.csv\")))\n",
    "    print(f\"Found {len(csv_files)} CSV file(s) in folder '{folder_path}'.\")\n",
    "\n",
    "    for index, csv_file in enumerate(csv_files, start=1):\n",
    "        if str(index) in results:\n",
    "            print(f\"Skipping file {index} ('{csv_file}') - already in JSON results.\")\n",
    "            continue\n",
    "        \n",
    "        descriptions = []\n",
    "        with open(csv_file, newline='', encoding='utf-8') as f:\n",
    "            reader = csv.DictReader(f, delimiter=\",\")\n",
    "            for row_number, row in enumerate(reader, start=1):\n",
    "                if \"Description\" in row:\n",
    "                    description = row[\"Description\"]\n",
    "                    descriptions.append(description)\n",
    "                else:\n",
    "                    print(\n",
    "                        f\"Warning: 'Description' column not found in row {row_number} of {csv_file}\"\n",
    "                    )\n",
    "\n",
    "        prompt = (\n",
    "            \"I have a list of scenes describing a movie step by step. \"\n",
    "            \"Please transform them into a single paragraph, preserving the chronological order. \"\n",
    "            \"Include every detail from each scene without adding or omitting any information. \"\n",
    "            \"Use only straightforward, factual wording, and avoid any new or descriptive language beyond what is provided in the scene notes.\\n\\n\"\n",
    "            \"Input: \\tDescription\\n\"\n",
    "        )\n",
    "        for i, desc in enumerate(descriptions, start=1):\n",
    "            prompt += f\"{i}\\t{desc}\\n\"\n",
    "\n",
    "        attempt = 0\n",
    "        while attempt < MAX_RETRIES:\n",
    "            try:\n",
    "                response = get_claude_response(prompt)\n",
    "                break\n",
    "            except Exception as e:\n",
    "                attempt += 1\n",
    "                if attempt < MAX_RETRIES:\n",
    "                    time.sleep(3)\n",
    "                else:\n",
    "                    results[str(index)] = f\"Error after {MAX_RETRIES} attempts: {e}\"\n",
    "                    with open(output_json_file, \"w\", encoding='utf-8') as json_file:\n",
    "                        json.dump(results, json_file, indent=4, ensure_ascii=False)\n",
    "                    sys.exit(1)\n",
    "\n",
    "        results[str(index)] = response\n",
    "        with open(output_json_file, \"w\", encoding='utf-8') as json_file:\n",
    "            json.dump(results, json_file, indent=4, ensure_ascii=False)\n",
    "        print(f\"Progress saved after file {index}.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    folder_path = \"/home/jacks.local/hdubey/VADv6/experiment/rawGT3\"\n",
    "    output_json_file = \"GT_claude.json\"\n",
    "\n",
    "    print(\"Starting CSV processing and Claude querying...\")\n",
    "    process_csv_files(folder_path, output_json_file)\n",
    "    print(\"Processing complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92081c56-24f5-4b38-9941-3b6a1e2688af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795cf7ae-8fad-4fda-afc4-068ae897ce1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3555f67c-6b19-476c-bdeb-8a828f434683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------\n",
    "# Base GT - Mistral\n",
    "# -------------------------------------------\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import csv\n",
    "import openai\n",
    "import json\n",
    "import sys\n",
    "import time\n",
    "\n",
    "openai.api_key = \"placeholder\"  # Replace with your actual API key\n",
    "\n",
    "MAX_RETRIES = 3  # Number of retries on API failure\n",
    "\n",
    "def get_mistral_response(prompt, model=\"mistral\", temperature=1, max_tokens=10000):\n",
    "    \"\"\"\n",
    "    Sends a prompt to Mistral and returns the response content or raises an Exception.\n",
    "    \"\"\"\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens,\n",
    "    )\n",
    "    content = response['choices'][0]['message']['content'].strip()\n",
    "    return content\n",
    "\n",
    "def process_csv_files(folder_path, output_json_file):\n",
    "    \"\"\"\n",
    "    Process all CSV files in a folder. For each CSV:\n",
    "      1) Read the 'Description' column in order.\n",
    "      2) Construct a single prompt.\n",
    "      3) Call Mistral with retries on error.\n",
    "      4) Save partial progress in a JSON file so the script can be resumed.\n",
    "    \"\"\"\n",
    "    if os.path.exists(output_json_file):\n",
    "        with open(output_json_file, \"r\", encoding=\"utf-8\") as jf:\n",
    "            try:\n",
    "                results = json.load(jf)\n",
    "            except json.JSONDecodeError:\n",
    "                results = {}\n",
    "        print(f\"Loaded existing results from '{output_json_file}'.\")\n",
    "    else:\n",
    "        results = {}\n",
    "        print(f\"No existing '{output_json_file}' found. Starting fresh.\")\n",
    "    \n",
    "    csv_files = sorted(glob.glob(os.path.join(folder_path, \"*.csv\")))\n",
    "    print(f\"Found {len(csv_files)} CSV file(s) in folder '{folder_path}'.\")\n",
    "\n",
    "    for index, csv_file in enumerate(csv_files, start=1):\n",
    "        if str(index) in results:\n",
    "            print(f\"Skipping file {index} ('{csv_file}') - already in JSON results.\")\n",
    "            continue\n",
    "        \n",
    "        descriptions = []\n",
    "        with open(csv_file, newline='', encoding='utf-8') as f:\n",
    "            reader = csv.DictReader(f, delimiter=\",\")\n",
    "            for row_number, row in enumerate(reader, start=1):\n",
    "                if \"Description\" in row:\n",
    "                    description = row[\"Description\"]\n",
    "                    descriptions.append(description)\n",
    "                else:\n",
    "                    print(\n",
    "                        f\"Warning: 'Description' column not found in row {row_number} of {csv_file}\"\n",
    "                    )\n",
    "\n",
    "        prompt = (\n",
    "            \"I have a list of scenes describing a movie step by step. \"\n",
    "            \"Please transform them into a single paragraph, preserving the chronological order. \"\n",
    "            \"Include every detail from each scene without adding or omitting any information. \"\n",
    "            \"Use only straightforward, factual wording, and avoid any new or descriptive language beyond what is provided in the scene notes.\\n\\n\"\n",
    "            \"Input: \\tDescription\\n\"\n",
    "        )\n",
    "        for i, desc in enumerate(descriptions, start=1):\n",
    "            prompt += f\"{i}\\t{desc}\\n\"\n",
    "\n",
    "        attempt = 0\n",
    "        while attempt < MAX_RETRIES:\n",
    "            try:\n",
    "                response = get_mistral_response(prompt)\n",
    "                break\n",
    "            except Exception as e:\n",
    "                attempt += 1\n",
    "                if attempt < MAX_RETRIES:\n",
    "                    time.sleep(3)\n",
    "                else:\n",
    "                    results[str(index)] = f\"Error after {MAX_RETRIES} attempts: {e}\"\n",
    "                    with open(output_json_file, \"w\", encoding='utf-8') as json_file:\n",
    "                        json.dump(results, json_file, indent=4, ensure_ascii=False)\n",
    "                    sys.exit(1)\n",
    "\n",
    "        results[str(index)] = response\n",
    "        with open(output_json_file, \"w\", encoding='utf-8') as json_file:\n",
    "            json.dump(results, json_file, indent=4, ensure_ascii=False)\n",
    "        print(f\"Progress saved after file {index}.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    folder_path = \"/home/jacks.local/hdubey/VADv6/experiment/rawGT3\"\n",
    "    output_json_file = \"GT_mistral.json\"\n",
    "\n",
    "    print(\"Starting CSV processing and Mistral querying...\")\n",
    "    process_csv_files(folder_path, output_json_file)\n",
    "    print(\"Processing complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3661844d-2115-4899-a93c-368aff05224c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c8c171-1442-4e9c-8d49-464c841dbe91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae86a55-1662-4702-8ffc-9a4ffcf78a6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d43e7ff-b5a9-422a-b87f-83d80fe3e780",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8858c4f0-a09e-448b-8c34-afecab0e41c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353f29c7-7e29-436d-8d1b-87ae3f7f7e03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de48ac8-05af-4192-8a7f-dff67b4e3c37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016de264-3c77-4815-b4b1-470ca1a043f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb646e1c-a520-40a3-8e1d-ccd70557954a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1432c24d-5f46-48cc-a738-beb4507df883",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7dd896b-262d-484d-8d87-ebcfcab0324a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524d79b9-1642-4668-b56b-237d916115e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5612afeb-0f64-4917-b93b-38b932ec5521",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3266b1a5-e4a5-49fb-be2e-5116315b72ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769b1ea9-d2a0-4a24-b8a8-4d73dd2d1672",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac4e4b5-8913-46b8-8345-ed47886ef722",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21c69da-064a-48d7-b4c9-3211c2ceab9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1912c7-6e3c-42c6-bf53-c6f181652ef6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c37d58f-7161-41e8-bb24-b398e34db596",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99d86c2-9034-4d56-8675-6ff135b2e86f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b50996-1ea2-4e06-930c-cbe98118144f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======================================\n",
    "# Summarization\n",
    "# ======================================\n",
    "\n",
    "import os\n",
    "import json\n",
    "import openai\n",
    "import time\n",
    "\n",
    "\n",
    "# Specify the folder where you want to save the iterative summarized outputs.\n",
    "GT_FILE_PATH = \"/mmfs1/scratch/jacks.local/mali9292/VAD/data/GroundTruths/GT_part1_25.json\"  # Ground Truth File Path\n",
    "OUTPUT_DIR = \"/mmfs1/scratch/jacks.local/mali9292/VAD/data/summarization\"  # <-- UPDATE THIS PATH\n",
    "\n",
    "MODEL = \"gpt-4o\"  # or \"gpt4o\" if that is your model name\n",
    "TEMPERATURE = 0.9\n",
    "MAX_TOKENS = 5000\n",
    "\n",
    "# Iterative summarization parameters:\n",
    "TARGET_PERCENT = 90  # Change this to 80 (or any other value) to target a different word count ratio.\n",
    "NUM_ITERATIONS = 6   # Number of iterative summarization cycles\n",
    "\n",
    "# ============================\n",
    "# PROMPT TEMPLATE\n",
    "# ============================\n",
    "\n",
    "# This prompt template uses {target_percent} (e.g., 90) and {text} (the description to summarize)\n",
    "PROMPT_TEMPLATE = (\n",
    "    \"Summarize the video description below so that it’s about {target_percent}% of the original word count. \"\n",
    "    \"First, identify the core plot by highlighting the main characters, their motivations, and the central conflict. \"\n",
    "    \"Next, pinpoint the essential events such as the inciting incident, climax, and resolution. \"\n",
    "    \"Remove or condense any less crucial details—if there are lengthy descriptions or minor character moments that don’t significantly affect the main narrative, trim them down. \"\n",
    "    \"If subplots influence the protagonist’s choices, include them briefly. Combine related sentences into a few succinct lines and use direct, concise language by eliminating filler words and repetition. \"\n",
    "    \"Ensure the summary flows logically and maintains cohesion without any gaps that might confuse the reader. \"\n",
    "    \"Finally, check the word count and aim for around {target_percent}% of the original, making slight adjustments as needed.\\n\\n\"\n",
    "    \"For example, consider the following original description (89 words):\\n\"\n",
    "    \"Amelia, a dedicated environmental activist, embarks on a perilous journey to rescue her coastal hometown from looming ecological ruin. \"\n",
    "    \"When a powerful corporation’s reckless actions threaten to pollute pristine shores and decimate marine life, she unites a diverse coalition of locals, scientists, and activists. \"\n",
    "    \"As protests escalate and sacrifices mount, unexpected alliances form and old rivalries resurface. \"\n",
    "    \"Amelia’s unwavering determination to secure a sustainable future becomes a beacon of hope amid growing chaos and environmental despair. \"\n",
    "    \"Her journey tests both her courage and her commitment to the cause.\\n\\n\"\n",
    "    \"A transformed summary at approximately {target_percent}% of the original word count (80 words) might read:\\n\"\n",
    "    \"Amelia, an environmental activist, sets out on a risky mission to save her coastal hometown from impending ecological collapse. \"\n",
    "    \"Faced with a corporation’s reckless actions endangering pristine shores and marine life, she forms a coalition of locals, scientists, and fellow activists. \"\n",
    "    \"As protests escalate and sacrifices increase, unexpected alliances and revived rivalries emerge. \"\n",
    "    \"Her steadfast determination to achieve sustainability shines as a hopeful signal amid chaos, remarkably testing her courage and commitment to the cause while inspiring community action.\\n\\n\"\n",
    "    \"Video Description:\\n{text}\"\n",
    ")\n",
    "\n",
    "# ============================\n",
    "# FUNCTION DEFINITIONS\n",
    "# ============================\n",
    "\n",
    "def get_summary(text, target_percent):\n",
    "    \"\"\"\n",
    "    Given a text and a target percentage, this function builds the prompt,\n",
    "    calls the OpenAI API to generate a summarized version, and returns the summary.\n",
    "    \"\"\"\n",
    "    prompt = PROMPT_TEMPLATE.format(target_percent=target_percent, text=text)\n",
    "    try:\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=MODEL,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=TEMPERATURE,\n",
    "            max_tokens=MAX_TOKENS\n",
    "        )\n",
    "        summary = response.choices[0].message.content.strip()\n",
    "        return summary\n",
    "    except Exception as e:\n",
    "        print(f\"Error during API call: {e}\")\n",
    "        return None\n",
    "\n",
    "def process_ground_truths(gt_file_path, output_dir, target_percent, num_iterations):\n",
    "    \"\"\"\n",
    "    Loads the ground truth file and, for each GT:\n",
    "      - Iteratively summarizes the text 'num_iterations' times,\n",
    "      - Each time reducing the text to approximately 'target_percent' of its length.\n",
    "    The original text and all iterations are saved in a JSON file named \"<gt_id>.json\" in the output directory.\n",
    "    \"\"\"\n",
    "    # Load the ground truth file.\n",
    "    try:\n",
    "        with open(gt_file_path, 'r', encoding='utf-8') as f:\n",
    "            ground_truths = json.load(f)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading ground truth file: {e}\")\n",
    "        return\n",
    "\n",
    "    # Ensure the output directory exists.\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Process each ground truth.\n",
    "    for gt_id, original_text in ground_truths.items():\n",
    "        print(f\"Processing GT ID: {gt_id}\")\n",
    "        result = {\n",
    "            \"original\": original_text,\n",
    "            \"iterations\": {}  # Will store each iteration summary as a string.\n",
    "        }\n",
    "        current_text = original_text  # Begin with the original text.\n",
    "        for i in range(1, num_iterations + 1):\n",
    "            print(f\"  Iteration {i}: Summarizing to {target_percent}% of current text length...\")\n",
    "            summary = get_summary(current_text, target_percent)\n",
    "            if summary is None:\n",
    "                summary = f\"Error generating summary at iteration {i}\"\n",
    "            result[\"iterations\"][str(i)] = summary\n",
    "            current_text = summary  # Use the summary for the next iteration.\n",
    "            time.sleep(1)  # Optional pause between API calls to respect rate limits.\n",
    "\n",
    "        # Save the result for this GT in a JSON file (e.g., \"1.json\" for GT with key \"1\").\n",
    "        output_path = os.path.join(output_dir, f\"{gt_id}.json\")\n",
    "        try:\n",
    "            with open(output_path, 'w', encoding='utf-8') as out_f:\n",
    "                json.dump(result, out_f, indent=2, ensure_ascii=False)\n",
    "            print(f\"  Saved iterative summaries to {output_path}\\n\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving file {output_path}: {e}\")\n",
    "\n",
    "# ============================\n",
    "# MAIN EXECUTION\n",
    "# ============================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process_ground_truths(GT_FILE_PATH, OUTPUT_DIR, TARGET_PERCENT, NUM_ITERATIONS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "880dbb14-24a9-4744-8d88-bf8f8f0edccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing GT ID: 1\n",
      "Processing GT ID: 2\n",
      "Processing GT ID: 3\n",
      "Processing GT ID: 4\n",
      "Processing GT ID: 5\n",
      "Processing GT ID: 6\n",
      "Processing GT ID: 7\n",
      "Processing GT ID: 8\n",
      "Processing GT ID: 9\n",
      "Processing GT ID: 10\n",
      "\n",
      "All done!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ======================================\n",
    "# Addition\n",
    "# ======================================\n",
    "\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import random\n",
    "import torch\n",
    "import re\n",
    "from wtpsplit import SaT  # <-- from your snippet\n",
    "\n",
    "# ----------------------------\n",
    "# Configuration\n",
    "# ----------------------------\n",
    "GT_FILE_PATH = \"/mmfs1/scratch/jacks.local/mali9292/VAD/data/GroundTruths/MPII_GT_10.json\"\n",
    "BASE_OUTPUT_DIR = \"/mmfs1/scratch/jacks.local/mali9292/VAD/data/addition\"\n",
    "\n",
    "NUM_ITERATIONS = 7\n",
    "UNRELATED_SEGMENTS = [\n",
    "    \"In the neon-lit back alleys of a dystopian metropolis, a rogue hacker races against time while being chased by relentless cyborg enforcers, their footsteps echoing off rain-soaked pavement.\",\n",
    "    \"Under the relentless desert sun, a lone gunslinger confronts a notorious outlaw at a deserted crossroads, both men exchanging steely glances as swirling dust blurs the horizon.\",\n",
    "    \"On a foggy evening in Victorian London, an intrepid detective uncovers a hidden conspiracy among the city’s elite, with gaslit streets setting the stage for a race against a mounting darkness.\",\n",
    "    \"High above a sprawling city skyline, a daredevil pilot executes death-defying maneuvers in a vintage biplane, narrowly evading enemy fire as the ground morphs into a dizzying mosaic below.\",\n",
    "    \"Within the creaking walls of an old countryside manor, an estranged family gathers for a mysterious inheritance, their hushed whispers and secret glances weaving a tapestry of betrayal and suspense.\",\n",
    "    \"Amidst the roaring tempest of a stormy ocean, a weathered sailor battles nature’s fury on a creaking vessel, his resolve as steadfast as the crashing waves that threaten to engulf him.\",\n",
    "    \"On a bustling 1960s New York street, a passionate artist finds unexpected inspiration in the chaotic interplay of urban life, capturing moments of love, loss, and defiant hope on a rain-soaked canvas.\",\n",
    "    \"Deep in the heart of an enchanted forest, a brave knight embarks on a perilous quest to rescue a captive princess, facing mythical creatures and treacherous traps with unwavering courage.\",\n",
    "    \"During a heated political rally in a futuristic city, a charismatic leader stokes both hope and dissent as holographic banners illuminate the night sky and fervent crowds surge with anticipation.\",\n",
    "    \"In the eerie silence of a post-apocalyptic wasteland, a hardened survivor scavenges through abandoned ruins, haunted by the echoes of a once-thriving civilization now reduced to dust.\",\n",
    "    \"Beneath the shimmering surface of an underwater kingdom, a rebellious mermaid defies ancient traditions to explore forbidden coral reefs and secret caverns brimming with forgotten lore.\",\n",
    "    \"In the quiet confines of a suburban attic, an unassuming teenager stumbles upon a mysterious portal that thrusts him into a surreal world where the very fabric of time and space unravels.\",\n",
    "    \"On the rugged highlands of Scotland, a stoic warrior faces rival clans and ancient curses alike, his fierce battle cry resonating across mist-covered moors and turbulent skies.\",\n",
    "    \"Amid the vibrant chaos of a South American carnival, a fearless dancer twirls through streets bursting with color and rhythm, each graceful move challenging the rigid boundaries of tradition.\",\n",
    "    \"During a clandestine meeting in a snow-covered mountain lodge, two rival spies exchange cryptic messages hinting at an imminent global conspiracy, their whispered words carried off by the howling wind.\",\n",
    "    \"Inside a lavish 1920s speakeasy, a troubled jazz musician pours his soul into a melancholic melody, every note echoing the bittersweet memories of lost love and shattered dreams.\",\n",
    "    \"In a futuristic space station orbiting a distant planet, an intrepid crew confronts bizarre alien phenomena and an enigmatic interstellar threat, pushing the limits of human endurance.\",\n",
    "    \"Amidst the chaos of a raging civil war, a compassionate medic races through bombed-out streets, his every desperate step a race against time to save lives in the midst of utter devastation.\",\n",
    "    \"On a sun-dappled afternoon in a quaint European village, an eccentric inventor unveils a groundbreaking contraption that defies conventional physics, igniting wonder and skepticism in equal measure.\",\n",
    "    \"In a bustling marketplace in ancient Persia, a cunning street thief navigates labyrinthine alleyways and vibrant bazaars, his calculated every move a daring escape from the ever-watchful royal guards.\"\n",
    "]\n",
    "PERCENTAGE = 10.0  # i.e. 10%\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 1) Load SaT model (from your snippet)\n",
    "# ----------------------------------------------------\n",
    "sat_adapted = SaT(\"sat-12l-sm\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "sat_adapted.half().to(device)\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 2) Helper Functions (unchanged except for SaT usage)\n",
    "# ----------------------------------------------------\n",
    "\n",
    "def ensure_dir(path):\n",
    "    \"\"\"Create directory if it doesn't exist.\"\"\"\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "def clean_trailing_punctuation(segment: str) -> str:\n",
    "    \"\"\"\n",
    "    - Removes repeated trailing dots (turning '...' or '..' into a single '.').\n",
    "    - Similarly normalizes repeated '!' or '?' to a single character.\n",
    "    - Leaves other punctuation alone (commas, quotes, etc.)\n",
    "    \"\"\"\n",
    "    segment = segment.strip()\n",
    "    # Convert any trailing sequence of '.' into a single '.'\n",
    "    segment = re.sub(r'[.]+$', '.', segment)\n",
    "    # Convert any trailing sequence of '!' into a single '!'\n",
    "    segment = re.sub(r'[!]+$', '!', segment)\n",
    "    # Convert any trailing sequence of '?' into a single '?'\n",
    "    segment = re.sub(r'[?]+$', '?', segment)\n",
    "    return segment\n",
    "\n",
    "def join_segments(segments):\n",
    "    \"\"\"\n",
    "    Re-joins your SaT segments into a single text:\n",
    "      1) Clean up each segment’s trailing punctuation (no '...' or '..').\n",
    "      2) If there's NO punctuation at the end, add one period '.'.\n",
    "      3) Join them with a single space in between.\n",
    "    \"\"\"\n",
    "    cleaned = []\n",
    "    for seg in segments:\n",
    "        seg = clean_trailing_punctuation(seg)\n",
    "        if seg and seg[-1] not in {'.', '!', '?'}:\n",
    "            seg += '.'\n",
    "        cleaned.append(seg)\n",
    "    return ' '.join(cleaned)\n",
    "\n",
    "def add_unrelated_segments_at_beginning(text, unrelated_segments, sat_model, percentage=10.0):\n",
    "    \"\"\"\n",
    "    1. Split text into semantic segments with sat_model.\n",
    "    2. Calculate how many segments to add from unrelated_segments (randomly).\n",
    "    3. Insert them at the beginning.\n",
    "    4. Return the rejoined text.\n",
    "    \"\"\"\n",
    "    current_segments = sat_model.split(text)\n",
    "    if not current_segments:\n",
    "        return text\n",
    "\n",
    "    num_to_add = math.ceil(len(current_segments) * (percentage / 100.0))\n",
    "    chosen = random.sample(unrelated_segments, k=min(num_to_add, len(unrelated_segments)))\n",
    "\n",
    "    final_segments = chosen + current_segments\n",
    "    return join_segments(final_segments)\n",
    "\n",
    "def add_unrelated_segments_in_middle(text, unrelated_segments, sat_model, percentage=10.0):\n",
    "    \"\"\"\n",
    "    Same as above, but insert the chosen segments in the middle.\n",
    "    \"\"\"\n",
    "    current_segments = sat_model.split(text)\n",
    "    if not current_segments:\n",
    "        return text\n",
    "\n",
    "    num_to_add = math.ceil(len(current_segments) * (percentage / 100.0))\n",
    "    chosen = random.sample(unrelated_segments, k=min(num_to_add, len(unrelated_segments)))\n",
    "\n",
    "    mid_index = len(current_segments) // 2\n",
    "    final_segments = current_segments[:mid_index] + chosen + current_segments[mid_index:]\n",
    "    return join_segments(final_segments)\n",
    "\n",
    "def add_unrelated_segments_at_end(text, unrelated_segments, sat_model, percentage=10.0):\n",
    "    \"\"\"\n",
    "    Same as above, but insert the chosen segments at the end.\n",
    "    \"\"\"\n",
    "    current_segments = sat_model.split(text)\n",
    "    if not current_segments:\n",
    "        return text\n",
    "\n",
    "    num_to_add = math.ceil(len(current_segments) * (percentage / 100.0))\n",
    "    chosen = random.sample(unrelated_segments, k=min(num_to_add, len(unrelated_segments)))\n",
    "\n",
    "    final_segments = current_segments + chosen\n",
    "    return join_segments(final_segments)\n",
    "\n",
    "def add_unrelated_segments_randomly(text, unrelated_segments, sat_model, percentage=10.0):\n",
    "    \"\"\"\n",
    "    1. Split text into semantic segments with sat_model.\n",
    "    2. Calculate how many segments to add from unrelated_segments (randomly).\n",
    "    3. Insert them at random positions within the existing segments.\n",
    "    4. Return the rejoined text.\n",
    "    \"\"\"\n",
    "    current_segments = sat_model.split(text)\n",
    "    if not current_segments:\n",
    "        return text\n",
    "\n",
    "    num_to_add = math.ceil(len(current_segments) * (percentage / 100.0))\n",
    "    chosen_segments = random.sample(unrelated_segments, k=min(num_to_add, len(unrelated_segments)))\n",
    "\n",
    "    final_segments = list(current_segments) # create a mutable list\n",
    "    insertion_points = sorted(random.sample(range(len(current_segments) + 1), k=min(num_to_add, len(current_segments) + 1))) # insertion points, sorted to insert correctly\n",
    "\n",
    "    inserted_count = 0\n",
    "    for i, insert_point in enumerate(insertion_points):\n",
    "        final_segments.insert(insert_point + inserted_count, chosen_segments[i])\n",
    "        inserted_count += 1 # Increment for each inserted segment\n",
    "\n",
    "    return join_segments(final_segments)\n",
    "\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 3) Main Logic\n",
    "# ----------------------------------------------------\n",
    "\n",
    "def main():\n",
    "    random.seed(42)  # for reproducible results; remove if you want fully random\n",
    "\n",
    "    # 1) Read the ground truth file\n",
    "    try:\n",
    "        with open(GT_FILE_PATH, 'r', encoding='utf-8') as f:\n",
    "            ground_truths = json.load(f)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading ground truth file: {e}\")\n",
    "        return\n",
    "\n",
    "    # 2) Prepare subdirectories\n",
    "    begin_dir = os.path.join(BASE_OUTPUT_DIR, \"beginning\")\n",
    "    middle_dir = os.path.join(BASE_OUTPUT_DIR, \"middle\")\n",
    "    end_dir = os.path.join(BASE_OUTPUT_DIR, \"end\")\n",
    "    random_dir = os.path.join(BASE_OUTPUT_DIR, \"random\") # New directory for random insertion\n",
    "\n",
    "    ensure_dir(begin_dir)\n",
    "    ensure_dir(middle_dir)\n",
    "    ensure_dir(end_dir)\n",
    "    ensure_dir(random_dir) # Ensure new directory is created\n",
    "\n",
    "    # 3) Process each ground truth\n",
    "    for gt_id, original_text in ground_truths.items():\n",
    "        print(f\"Processing GT ID: {gt_id}\")\n",
    "\n",
    "        # ---- A) Beginning ----\n",
    "        begin_output = {\n",
    "            \"ground_truth\": original_text,\n",
    "            \"iterations\": {}\n",
    "        }\n",
    "        current_text = original_text\n",
    "        for i in range(1, NUM_ITERATIONS + 1):\n",
    "            current_text = add_unrelated_segments_at_beginning(\n",
    "                current_text,\n",
    "                UNRELATED_SEGMENTS,\n",
    "                sat_model=sat_adapted,\n",
    "                percentage=PERCENTAGE\n",
    "            )\n",
    "            begin_output[\"iterations\"][str(i)] = current_text\n",
    "\n",
    "        begin_file = os.path.join(begin_dir, f\"{gt_id}.json\")\n",
    "        with open(begin_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(begin_output, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "        # ---- B) Middle ----\n",
    "        middle_output = {\n",
    "            \"ground_truth\": original_text,\n",
    "            \"iterations\": {}\n",
    "        }\n",
    "        current_text = original_text\n",
    "        for i in range(1, NUM_ITERATIONS + 1):\n",
    "            current_text = add_unrelated_segments_in_middle(\n",
    "                current_text,\n",
    "                UNRELATED_SEGMENTS,\n",
    "                sat_model=sat_adapted,\n",
    "                percentage=PERCENTAGE\n",
    "            )\n",
    "            middle_output[\"iterations\"][str(i)] = current_text\n",
    "\n",
    "        middle_file = os.path.join(middle_dir, f\"{gt_id}.json\")\n",
    "        with open(middle_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(middle_output, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "        # ---- C) End ----\n",
    "        end_output = {\n",
    "            \"ground_truth\": original_text,\n",
    "            \"iterations\": {}\n",
    "        }\n",
    "        current_text = original_text\n",
    "        for i in range(1, NUM_ITERATIONS + 1):\n",
    "            current_text = add_unrelated_segments_at_end(\n",
    "                current_text,\n",
    "                UNRELATED_SEGMENTS,\n",
    "                sat_model=sat_adapted,\n",
    "                percentage=PERCENTAGE\n",
    "            )\n",
    "            end_output[\"iterations\"][str(i)] = current_text\n",
    "\n",
    "        end_file = os.path.join(end_dir, f\"{gt_id}.json\")\n",
    "        with open(end_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(end_output, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "        # ---- D) Random ---- # New category\n",
    "        random_output = {\n",
    "            \"ground_truth\": original_text,\n",
    "            \"iterations\": {}\n",
    "        }\n",
    "        current_text = original_text\n",
    "        for i in range(1, NUM_ITERATIONS + 1):\n",
    "            current_text = add_unrelated_segments_randomly( # Calling the new function\n",
    "                current_text,\n",
    "                UNRELATED_SEGMENTS,\n",
    "                sat_model=sat_adapted,\n",
    "                percentage=PERCENTAGE\n",
    "            )\n",
    "            random_output[\"iterations\"][str(i)] = current_text\n",
    "\n",
    "        random_file = os.path.join(random_dir, f\"{gt_id}.json\")\n",
    "        with open(random_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(random_output, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "\n",
    "    print(\"\\nAll done!\\n\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47fb0144-dc36-46a9-89b6-c0d9ab52d186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing GT ID: 1\n",
      "Processing GT ID: 2\n",
      "Processing GT ID: 3\n",
      "Processing GT ID: 4\n",
      "Processing GT ID: 5\n",
      "Processing GT ID: 6\n",
      "Processing GT ID: 7\n",
      "Processing GT ID: 8\n",
      "Processing GT ID: 9\n",
      "Processing GT ID: 10\n",
      "\n",
      "All done!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ======================================\n",
    "# Deletion\n",
    "# ======================================\n",
    "\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import random\n",
    "import torch\n",
    "import re\n",
    "from wtpsplit import SaT  # <-- from your snippet\n",
    "\n",
    "# ----------------------------\n",
    "# Configuration\n",
    "# ----------------------------\n",
    "GT_FILE_PATH = \"/mmfs1/scratch/jacks.local/mali9292/VAD/data/GroundTruths/MPII_GT_10.json\"\n",
    "BASE_OUTPUT_DIR = \"/mmfs1/scratch/jacks.local/mali9292/VAD/data/deletion\" # Changed to deletion\n",
    "\n",
    "NUM_ITERATIONS = 7\n",
    "PERCENTAGE = 10.0  # i.e. 10%\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 1) Load SaT model (from your snippet)\n",
    "# ----------------------------------------------------\n",
    "sat_adapted = SaT(\"sat-12l-sm\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "sat_adapted.half().to(device)\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 2) Helper Functions (unchanged except for SaT usage)\n",
    "# ----------------------------------------------------\n",
    "\n",
    "def ensure_dir(path):\n",
    "    \"\"\"Create directory if it doesn't exist.\"\"\"\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "def clean_trailing_punctuation(segment: str) -> str:\n",
    "    \"\"\"\n",
    "    - Removes repeated trailing dots (turning '...' or '..' into a single '.').\n",
    "    - Similarly normalizes repeated '!' or '?' to a single character.\n",
    "    - Leaves other punctuation alone (commas, quotes, etc.)\n",
    "    \"\"\"\n",
    "    segment = segment.strip()\n",
    "    # Convert any trailing sequence of '.' into a single '.'\n",
    "    segment = re.sub(r'[.]+$', '.', segment)\n",
    "    # Convert any trailing sequence of '!' into a single '!'\n",
    "    segment = re.sub(r'[!]+$', '!', segment)\n",
    "    # Convert any trailing sequence of '?' into a single '?'\n",
    "    segment = re.sub(r'[?]+$', '?', segment)\n",
    "    return segment\n",
    "\n",
    "def join_segments(segments):\n",
    "    \"\"\"\n",
    "    Re-joins your SaT segments into a single text:\n",
    "      1) Clean up each segment’s trailing punctuation (no '...' or '..').\n",
    "      2) If there's NO punctuation at the end, add one period '.'.\n",
    "      3) Join them with a single space in between.\n",
    "    \"\"\"\n",
    "    cleaned = []\n",
    "    for seg in segments:\n",
    "        seg = clean_trailing_punctuation(seg)\n",
    "        if seg and seg[-1] not in {'.', '!', '?'}:\n",
    "            seg += '.'\n",
    "        cleaned.append(seg)\n",
    "    return ' '.join(cleaned)\n",
    "\n",
    "def delete_segments_at_beginning(text, sat_model, percentage=10.0):\n",
    "    \"\"\"\n",
    "    1. Split text into semantic segments with sat_model.\n",
    "    2. Calculate how many segments to delete.\n",
    "    3. Delete segments from the beginning.\n",
    "    4. Return the rejoined text.\n",
    "    \"\"\"\n",
    "    current_segments = sat_model.split(text)\n",
    "    if not current_segments:\n",
    "        return text\n",
    "\n",
    "    num_to_delete = math.ceil(len(current_segments) * (percentage / 100.0))\n",
    "    if num_to_delete >= len(current_segments): # Avoid deleting all segments\n",
    "        num_to_delete = len(current_segments) - 1 if len(current_segments) > 0 else 0\n",
    "    segments_to_keep = current_segments[num_to_delete:]\n",
    "    return join_segments(segments_to_keep)\n",
    "\n",
    "def delete_segments_in_middle(text, sat_model, percentage=10.0):\n",
    "    \"\"\"\n",
    "    Same as above, but delete segments from the middle.\n",
    "    \"\"\"\n",
    "    current_segments = sat_model.split(text)\n",
    "    if not current_segments:\n",
    "        return text\n",
    "\n",
    "    num_to_delete = math.ceil(len(current_segments) * (percentage / 100.0))\n",
    "    if num_to_delete >= len(current_segments): # Avoid deleting all segments\n",
    "        num_to_delete = len(current_segments) - 1 if len(current_segments) > 0 else 0\n",
    "\n",
    "    mid_index = len(current_segments) // 2\n",
    "    delete_start = mid_index - (num_to_delete // 2)\n",
    "    delete_end = delete_start + num_to_delete\n",
    "\n",
    "    # Adjust indices to stay within bounds\n",
    "    delete_start = max(0, delete_start)\n",
    "    delete_end = min(len(current_segments), delete_end)\n",
    "\n",
    "    segments_to_keep = current_segments[:delete_start] + current_segments[delete_end:]\n",
    "    return join_segments(segments_to_keep)\n",
    "\n",
    "def delete_segments_at_end(text, sat_model, percentage=10.0):\n",
    "    \"\"\"\n",
    "    Same as above, but delete segments from the end.\n",
    "    \"\"\"\n",
    "    current_segments = sat_model.split(text)\n",
    "    if not current_segments:\n",
    "        return text\n",
    "\n",
    "    num_to_delete = math.ceil(len(current_segments) * (percentage / 100.0))\n",
    "    if num_to_delete >= len(current_segments): # Avoid deleting all segments\n",
    "        num_to_delete = len(current_segments) - 1 if len(current_segments) > 0 else 0\n",
    "\n",
    "    segments_to_keep = current_segments[:-num_to_delete]\n",
    "    return join_segments(segments_to_keep)\n",
    "\n",
    "def delete_segments_randomly(text, sat_model, percentage=10.0):\n",
    "    \"\"\"\n",
    "    1. Split text into semantic segments with sat_model.\n",
    "    2. Calculate how many segments to delete.\n",
    "    3. Randomly choose segments to delete from within the existing segments.\n",
    "    4. Return the rejoined text.\n",
    "    \"\"\"\n",
    "    current_segments = sat_model.split(text)\n",
    "    if not current_segments:\n",
    "        return text\n",
    "\n",
    "    num_to_delete = math.ceil(len(current_segments) * (percentage / 100.0))\n",
    "    if num_to_delete >= len(current_segments): # Avoid deleting all segments\n",
    "        num_to_delete = len(current_segments) - 1 if len(current_segments) > 0 else 0\n",
    "\n",
    "    indices_to_delete = sorted(random.sample(range(len(current_segments)), k=num_to_delete), reverse=True) # Reverse to avoid index issues after deletion\n",
    "    segments_to_keep = list(current_segments) # Create a mutable list\n",
    "\n",
    "    for index in indices_to_delete:\n",
    "        segments_to_keep.pop(index)\n",
    "\n",
    "    return join_segments(segments_to_keep)\n",
    "\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 3) Main Logic\n",
    "# ----------------------------------------------------\n",
    "\n",
    "def main():\n",
    "    random.seed(42)  # for reproducible results; remove if you want fully random\n",
    "\n",
    "    # 1) Read the ground truth file\n",
    "    try:\n",
    "        with open(GT_FILE_PATH, 'r', encoding='utf-8') as f:\n",
    "            ground_truths = json.load(f)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading ground truth file: {e}\")\n",
    "        return\n",
    "\n",
    "    # 2) Prepare subdirectories\n",
    "    begin_dir = os.path.join(BASE_OUTPUT_DIR, \"beginning\")\n",
    "    middle_dir = os.path.join(BASE_OUTPUT_DIR, \"middle\")\n",
    "    end_dir = os.path.join(BASE_OUTPUT_DIR, \"end\")\n",
    "    random_dir = os.path.join(BASE_OUTPUT_DIR, \"random\") # New directory for random deletion\n",
    "\n",
    "    ensure_dir(begin_dir)\n",
    "    ensure_dir(middle_dir)\n",
    "    ensure_dir(end_dir)\n",
    "    ensure_dir(random_dir) # Ensure new directory is created\n",
    "\n",
    "    # 3) Process each ground truth\n",
    "    for gt_id, original_text in ground_truths.items():\n",
    "        print(f\"Processing GT ID: {gt_id}\")\n",
    "\n",
    "        # ---- A) Beginning ----\n",
    "        begin_output = {\n",
    "            \"ground_truth\": original_text,\n",
    "            \"iterations\": {}\n",
    "        }\n",
    "        current_text = original_text\n",
    "        for i in range(1, NUM_ITERATIONS + 1):\n",
    "            current_text = delete_segments_at_beginning( # Calling the delete function\n",
    "                current_text,\n",
    "                sat_model=sat_adapted,\n",
    "                percentage=PERCENTAGE\n",
    "            )\n",
    "            begin_output[\"iterations\"][str(i)] = current_text\n",
    "\n",
    "        begin_file = os.path.join(begin_dir, f\"{gt_id}.json\")\n",
    "        with open(begin_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(begin_output, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "        # ---- B) Middle ----\n",
    "        middle_output = {\n",
    "            \"ground_truth\": original_text,\n",
    "            \"iterations\": {}\n",
    "        }\n",
    "        current_text = original_text\n",
    "        for i in range(1, NUM_ITERATIONS + 1):\n",
    "            current_text = delete_segments_in_middle( # Calling the delete function\n",
    "                current_text,\n",
    "                sat_model=sat_adapted,\n",
    "                percentage=PERCENTAGE\n",
    "            )\n",
    "            middle_output[\"iterations\"][str(i)] = current_text\n",
    "\n",
    "        middle_file = os.path.join(middle_dir, f\"{gt_id}.json\")\n",
    "        with open(middle_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(middle_output, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "        # ---- C) End ----\n",
    "        end_output = {\n",
    "            \"ground_truth\": original_text,\n",
    "            \"iterations\": {}\n",
    "        }\n",
    "        current_text = original_text\n",
    "        for i in range(1, NUM_ITERATIONS + 1):\n",
    "            current_text = delete_segments_at_end( # Calling the delete function\n",
    "                current_text,\n",
    "                sat_model=sat_adapted,\n",
    "                percentage=PERCENTAGE\n",
    "            )\n",
    "            end_output[\"iterations\"][str(i)] = current_text\n",
    "\n",
    "        end_file = os.path.join(end_dir, f\"{gt_id}.json\")\n",
    "        with open(end_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(end_output, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "        # ---- D) Random ---- # New category for deletion\n",
    "        random_output = {\n",
    "            \"ground_truth\": original_text,\n",
    "            \"iterations\": {}\n",
    "        }\n",
    "        current_text = original_text\n",
    "        for i in range(1, NUM_ITERATIONS + 1):\n",
    "            current_text = delete_segments_randomly( # Calling the random delete function\n",
    "                current_text,\n",
    "                sat_model=sat_adapted,\n",
    "                percentage=PERCENTAGE\n",
    "            )\n",
    "            random_output[\"iterations\"][str(i)] = current_text\n",
    "\n",
    "        random_file = os.path.join(random_dir, f\"{gt_id}.json\")\n",
    "        with open(random_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(random_output, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "\n",
    "    print(\"\\nAll done!\\n\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e60f841-992e-456d-a46b-472320f6c74f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing GT ID: 1\n",
      "  Generating version 1 of 4...\n",
      "  Generating version 2 of 4...\n",
      "  Generating version 3 of 4...\n",
      "  Generating version 4 of 4...\n",
      "  Saved sequel transformations to /mmfs1/scratch/jacks.local/mali9292/VAD/data/Versions/1.json\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import openai\n",
    "import time\n",
    "\n",
    "# ----------------------------\n",
    "# Configuration\n",
    "# ----------------------------\n",
    "MODEL = \"gpt-4o\"  # or \"gpt4o\" if that's your model identifier\n",
    "TEMPERATURE = 0.9\n",
    "MAX_TOKENS = 4000  # Adjust as needed; shorter than summarization since the prompt is simpler\n",
    "\n",
    "# ----------------------------\n",
    "# Prompt Template (Unchanged Except for the Original Text Placeholder)\n",
    "# ----------------------------\n",
    "SEQUEL_PROMPT_TEMPLATE = (\n",
    "    \"Transform the video description below into a sequel or continuation (Part 2) that mirrors the same setting, participants, objects, and sequence of actions—even using very similar wording—yet conveys a different story or perspective. The result should have:\\n\\n\"\n",
    "    \"High textual overlap: The same subjects, objects, and actions should appear in a similar order with similar phrases.\\n\"\n",
    "    \"A different storyline: Despite the overlap, the narrative or intent behind these actions must differ in some meaningful way (e.g., altered motivations, different emotional undercurrent, changed outcomes).\\n\"\n",
    "    \"No contradictions: The sequel shouldn't conflict with facts from the original; instead, it reuses the same framework with a fresh direction or twist in the story.\\n\"\n",
    "    \"Avoid adding completely new characters or new major actions—focus on reframing or recontextualizing the existing elements to produce a distinct Part 2.\\n\\n\"\n",
    "    \"Example\\n\"\n",
    "    \"Original Description (Part 1):\\n\"\n",
    "    \"He steps off the bus and checks his watch, noticing it's exactly 3 PM. Inside the nearby coffee shop, the barista waves at him. He smiles in response and orders a latte, taking a seat at the counter.\\n\\n\"\n",
    "    \"Transformed Sequel (Part 2, Same Actors/Objects/Sequence, Different Narrative):\\n\"\n",
    "    \"He steps off the bus again at precisely 3 PM, this time with a furrowed brow as he checks his watch. Inside the same coffee shop, the barista waves at him, but he barely returns the gesture and seems preoccupied. He orders a latte as usual, yet his tone carries a hint of impatience. Taking a seat at the counter, he taps his foot nervously, clearly anticipating something beyond the familiar comfort of his daily routine.\\n\\n\"\n",
    "    \"Notice how Part 2 reuses:\\n\\n\"\n",
    "    'The same subjects (\"He,\" the barista)\\n'\n",
    "    \"The same objects (the bus, the coffee shop, a latte, a seat)\\n\"\n",
    "    \"A similar order of actions (steps off the bus, checks watch, goes inside, orders latte, sits)\\n\"\n",
    "    'Nearly identical or synonymous wording (\"exactly 3 PM\" → \"precisely 3 PM,\" \"waves at him\" → \"the barista waves at him,\" etc.)\\n\\n'\n",
    "    \"…but establishes a different narrative tone and emotional context, resulting in a new story.\\n\\n Output Format: Only reply back with the transformed video description and nothing else.\\n\\n\"\n",
    "    \"Original Description (Part 1):\\n\"\n",
    "    \"{original_text}\\n\"\n",
    ")\n",
    "\n",
    "\n",
    "def generate_sequel(text):\n",
    "    \"\"\"\n",
    "    Given an original text, build the sequel/continuation prompt and call the OpenAI API\n",
    "    to generate a 'Part 2' story. Returns the GPT-4o response as a string.\n",
    "    \"\"\"\n",
    "    prompt = SEQUEL_PROMPT_TEMPLATE.format(original_text=text)\n",
    "    try:\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=MODEL,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=TEMPERATURE,\n",
    "            max_tokens=MAX_TOKENS\n",
    "        )\n",
    "        new_text = response.choices[0].message.content.strip()\n",
    "        # Post-process to remove unwanted labels in the output\n",
    "        return new_text\n",
    "    except Exception as e:\n",
    "        print(f\"Error during API call: {e}\")\n",
    "        return None\n",
    "\n",
    "def process_ground_truths(\n",
    "    gt_file_path,\n",
    "    output_dir,\n",
    "    n_versions=4,\n",
    "    selected_gt_ids=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Loads the ground truth file from 'gt_file_path' and, for each GT in that file:\n",
    "      1. Create n sequel transformations (versions) in an iterative way (each version is built from the previous).\n",
    "      2. Save the results in a file named '{gt_id}.json' in 'output_dir'.\n",
    "\n",
    "    Args:\n",
    "        gt_file_path (str): Path to the ground truth JSON file.\n",
    "        output_dir (str): Directory to save the resulting JSON files.\n",
    "        n_versions (int): Number of transformations to generate per ground truth.\n",
    "        selected_gt_ids (list or None): If provided, only process these GT IDs. If None or empty, process all.\n",
    "    \"\"\"\n",
    "    # 1. Load the ground-truth JSON file\n",
    "    try:\n",
    "        with open(gt_file_path, 'r', encoding='utf-8') as f:\n",
    "            ground_truths = json.load(f)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading ground truth file '{gt_file_path}': {e}\")\n",
    "        return\n",
    "\n",
    "    # If the user did not specify GT IDs to process, default to all\n",
    "    if not selected_gt_ids:\n",
    "        selected_gt_ids = ground_truths.keys()\n",
    "\n",
    "    # Ensure the output directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # 2. Iterate through each selected GT\n",
    "    for gt_id in selected_gt_ids:\n",
    "        if gt_id not in ground_truths:\n",
    "            print(f\"GT ID '{gt_id}' not found in the JSON file. Skipping.\")\n",
    "            continue\n",
    "        \n",
    "        original_text = ground_truths[gt_id]\n",
    "        print(f\"Processing GT ID: {gt_id}\")\n",
    "\n",
    "        # Prepare the structure to save\n",
    "        result = {\n",
    "            \"ground_truth\": original_text,\n",
    "            \"versions\": {}\n",
    "        }\n",
    "\n",
    "        current_text = original_text\n",
    "        # 3. Generate n versions, each sequel built from the previous\n",
    "        for i in range(1, n_versions + 1):\n",
    "            print(f\"  Generating version {i} of {n_versions}...\")\n",
    "            sequel_text = generate_sequel(current_text)\n",
    "            if not sequel_text:\n",
    "                sequel_text = f\"Error generating sequel at iteration {i}\"\n",
    "            result[\"versions\"][str(i)] = sequel_text\n",
    "            current_text = sequel_text  # Next iteration uses the newly generated text\n",
    "            time.sleep(1)  # Respect rate limits, adjust or remove as needed\n",
    "\n",
    "        # 4. Save results to a file named \"<gt_id>.json\"\n",
    "        output_path = os.path.join(output_dir, f\"{gt_id}.json\")\n",
    "        try:\n",
    "            with open(output_path, 'w', encoding='utf-8') as out_f:\n",
    "                json.dump(result, out_f, indent=2, ensure_ascii=False)\n",
    "            print(f\"  Saved sequel transformations to {output_path}\\n\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving file '{output_path}': {e}\")\n",
    "\n",
    "# ----------------------------\n",
    "# Example usage (if running as a script):\n",
    "# ----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # Example parameters (change these as needed)\n",
    "    GT_FILE_PATH = \"/mmfs1/scratch/jacks.local/mali9292/VAD/data/GroundTruths/MPII_GT_10.json\"\n",
    "    OUTPUT_DIR = \"/mmfs1/scratch/jacks.local/mali9292/VAD/data/Versions\"\n",
    "    N_VERSIONS = 4  # produce 4 sequel versions\n",
    "    SELECTED_GT_IDS = [\"1\"]\n",
    "\n",
    "    process_ground_truths(\n",
    "        gt_file_path=GT_FILE_PATH,\n",
    "        output_dir=OUTPUT_DIR,\n",
    "        n_versions=N_VERSIONS,\n",
    "        selected_gt_ids=SELECTED_GT_IDS\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17662777-9d4a-4940-b82b-948f56e3881a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b1883b-f621-478f-8f78-a039d2479615",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154558d8-96c3-4816-ba1f-7702542104eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Rotation with Head-Selection Truncation\n",
    "# Using sat_segmenter from your vad_lib\n",
    "# ============================================\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import re\n",
    "from vad_lib import sat_segmenter\n",
    "\n",
    "# --------------------------------\n",
    "# Global Config\n",
    "# --------------------------------\n",
    "GT_FILE_PATH = \"/mmfs1/scratch/jacks.local/mali9292/VAD/data/GroundTruths/MPII_GT_2.json\"\n",
    "BASE_OUTPUT_DIR = \"/mmfs1/scratch/jacks.local/mali9292/VAD/data/rotation\"\n",
    "\n",
    "NUM_ITERATIONS = 12\n",
    "NUMBER_OF_SEGMENTS = 12  # how many total segments to keep from the head\n",
    "ROTATE_SEGMENTS = 1      # how many segments to rotate each iteration (was %)\n",
    "\n",
    "# --------------------------------\n",
    "# Helper Functions\n",
    "# --------------------------------\n",
    "\n",
    "def ensure_dir(path):\n",
    "    \"\"\"Create a directory if it doesn't exist.\"\"\"\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "def clean_trailing_punctuation(segment: str) -> str:\n",
    "    \"\"\"\n",
    "    - Removes repeated trailing dots (turning '...' into '.').\n",
    "    - Similarly normalizes repeated '!' or '?' to a single character.\n",
    "    \"\"\"\n",
    "    segment = segment.strip()\n",
    "    segment = re.sub(r'[.]+$', '.', segment)\n",
    "    segment = re.sub(r'[!]+$', '!', segment)\n",
    "    segment = re.sub(r'[?]+$', '?', segment)\n",
    "    return segment\n",
    "\n",
    "def join_segments(segments):\n",
    "    \"\"\"\n",
    "    Re-joins your segments into a single text:\n",
    "      1) Clean trailing punctuation on each segment.\n",
    "      2) If there's no punctuation, add '.'.\n",
    "      3) Join them with a space.\n",
    "    \"\"\"\n",
    "    cleaned = []\n",
    "    for seg in segments:\n",
    "        seg = clean_trailing_punctuation(seg)\n",
    "        if seg and seg[-1] not in {'.', '!', '?'}:\n",
    "            seg += '.'\n",
    "        cleaned.append(seg)\n",
    "    return ' '.join(cleaned)\n",
    "\n",
    "def limit_to_fixed_segments(original_text, num_segments):\n",
    "    \"\"\"\n",
    "    1) Split into segments using sat_segmenter.\n",
    "    2) Keep only the first num_segments.\n",
    "    3) Re-join them.\n",
    "    \"\"\"\n",
    "    segments = sat_segmenter(original_text)\n",
    "    if len(segments) > num_segments:\n",
    "        segments = segments[:num_segments]\n",
    "    truncated_text = join_segments(segments)\n",
    "    return truncated_text\n",
    "\n",
    "def rotate_text_from_beginning(text, rotate_count=1):\n",
    "    \"\"\"\n",
    "    Rotate from the beginning:\n",
    "    - Move the first 'rotate_count' segments to the end.\n",
    "    \"\"\"\n",
    "    segments = sat_segmenter(text)\n",
    "    if not segments or rotate_count <= 0:\n",
    "        return text\n",
    "\n",
    "    # Make sure we don't rotate more segments than exist\n",
    "    chunk_size = min(rotate_count, len(segments))\n",
    "\n",
    "    chunk = segments[:chunk_size]\n",
    "    remainder = segments[chunk_size:]\n",
    "    rotated_segments = remainder + chunk\n",
    "    return join_segments(rotated_segments)\n",
    "\n",
    "def rotate_text_from_end(text, rotate_count=1):\n",
    "    \"\"\"\n",
    "    Rotate from the end:\n",
    "    - Move the last 'rotate_count' segments to the front.\n",
    "    \"\"\"\n",
    "    segments = sat_segmenter(text)\n",
    "    if not segments or rotate_count <= 0:\n",
    "        return text\n",
    "\n",
    "    chunk_size = min(rotate_count, len(segments))\n",
    "\n",
    "    chunk = segments[-chunk_size:]\n",
    "    remainder = segments[:-chunk_size]\n",
    "    rotated_segments = chunk + remainder\n",
    "    return join_segments(rotated_segments)\n",
    "\n",
    "# --------------------------------\n",
    "# Main Logic\n",
    "# --------------------------------\n",
    "\n",
    "def main():\n",
    "    # 1) Load original ground truths\n",
    "    try:\n",
    "        with open(GT_FILE_PATH, 'r', encoding='utf-8') as f:\n",
    "            ground_truths = json.load(f)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading GT file: {e}\")\n",
    "        return\n",
    "\n",
    "    # 2) Prepare output folders\n",
    "    beginning_dir = os.path.join(BASE_OUTPUT_DIR, \"beginning\")\n",
    "    end_dir = os.path.join(BASE_OUTPUT_DIR, \"end\")\n",
    "    ensure_dir(beginning_dir)\n",
    "    ensure_dir(end_dir)\n",
    "\n",
    "    # 3) For each GT, truncate and rotate\n",
    "    for gt_id, original_text in ground_truths.items():\n",
    "        print(f\"Processing GT ID: {gt_id}\")\n",
    "\n",
    "        # (A) Truncate to NUMBER_OF_SEGMENTS\n",
    "        truncated_gt = limit_to_fixed_segments(original_text, NUMBER_OF_SEGMENTS)\n",
    "\n",
    "        # (B) Rotate from Beginning\n",
    "        begin_output = {\n",
    "            \"ground_truth\": truncated_gt,  # truncated version\n",
    "            \"iterations\": {}\n",
    "        }\n",
    "        current_text = truncated_gt\n",
    "        for i in range(1, NUM_ITERATIONS + 1):\n",
    "            current_text = rotate_text_from_beginning(\n",
    "                current_text,\n",
    "                rotate_count=ROTATE_SEGMENTS\n",
    "            )\n",
    "            begin_output[\"iterations\"][str(i)] = current_text\n",
    "\n",
    "        begin_file = os.path.join(beginning_dir, f\"{gt_id}.json\")\n",
    "        with open(begin_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(begin_output, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "        # (C) Rotate from End\n",
    "        end_output = {\n",
    "            \"ground_truth\": truncated_gt,  # truncated version\n",
    "            \"iterations\": {}\n",
    "        }\n",
    "        current_text = truncated_gt\n",
    "        for i in range(1, NUM_ITERATIONS + 1):\n",
    "            current_text = rotate_text_from_end(\n",
    "                current_text,\n",
    "                rotate_count=ROTATE_SEGMENTS\n",
    "            )\n",
    "            end_output[\"iterations\"][str(i)] = current_text\n",
    "\n",
    "        end_file = os.path.join(end_dir, f\"{gt_id}.json\")\n",
    "        with open(end_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(end_output, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(\"\\nFinished rotation with truncation!\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f5c890-41b6-4d3f-8f88-a1567c067d6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (VAD)",
   "language": "python",
   "name": "vad"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
